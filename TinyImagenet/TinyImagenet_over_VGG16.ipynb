{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TinyImagenet_over_VGG16.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8xB_nS-iico",
        "colab_type": "text"
      },
      "source": [
        "# Training TinyImagenet over VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeX-5UojiZjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJrQYKTAjkpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras_contrib.callbacks import CyclicLR\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uars9Oh7jpqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 200\n",
        "epochs = 48\n",
        "num_train = 100000\n",
        "num_validation = 10000\n",
        "img_height = 64\n",
        "img_width = 64\n",
        "channels = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ5p1yPajq4U",
        "colab_type": "text"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLggDvlEjsjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "\n",
        "input_shape = (64, 64, 3)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(64, (3, 3), input_shape=input_shape, padding='same',\n",
        "           activation='relu'),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same',),\n",
        "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
        "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
        "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
        "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
        "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(4096, activation='relu'),\n",
        "    Dense(4096, activation='relu'),\n",
        "    Dense(200, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niFhNcURjuY7",
        "colab_type": "text"
      },
      "source": [
        "# Load Dataset & Pre-process\n",
        "\n",
        "### Dataset: [TinyImagenet](https://tiny-imagenet.herokuapp.com/)\n",
        "\n",
        "###  Note: Generate CCP attacked images and include in the training images for Augmented CCP training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7xs4sBcjuMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read Validation Dataset\n",
        "val_data = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, \n",
        "                       names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "val_data.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN-0ZipWkxS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining Customized Imagedatagenerator using imgaug library\n",
        "def CustomImageDataGen(input_img):\n",
        "  # Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n",
        "  # e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second\n",
        "  # image.\n",
        "  sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "  \n",
        "  seq = iaa.Sequential([\n",
        "      iaa.Fliplr(0.5), # horizontal flips\n",
        "      iaa.Flipud(0.2), # vertical flips\n",
        "      \n",
        "      # Small gaussian blur with random sigma between 0 and 0.5.\n",
        "      # But we only blur about 50% of all images.\n",
        "      sometimes(iaa.GaussianBlur(sigma=(0, 2.0))),\n",
        "      \n",
        "      # crop images by -10% to 20% of their height/width\n",
        "      sometimes(iaa.CropAndPad(\n",
        "          percent=(-0.1, 0.2),\n",
        "          pad_mode=ia.ALL,\n",
        "          pad_cval=(0, 255)\n",
        "        )),\n",
        "      \n",
        "      # Apply affine transformations to some of the images\n",
        "      # - scale to 80-120% of image height/width (each axis independently)\n",
        "      # - translate by -20 to +20 relative to height/width (per axis)\n",
        "      # - rotate by -45 to +45 degrees\n",
        "      # - shear by -16 to +16 degrees\n",
        "      # - order: use nearest neighbour or bilinear interpolation (fast)\n",
        "      # - mode: use any available mode to fill newly created pixels\n",
        "      #         see API or scikit-image for which modes are available\n",
        "      # - cval: if the mode is constant, then use a random brightness\n",
        "      #         for the newly created pixels (e.g. sometimes black,\n",
        "      #         sometimes white)\n",
        "      sometimes(iaa.Affine(\n",
        "          scale={\"x\": (0.8, 1.5), \"y\": (0.8, 1.5)},\n",
        "          translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
        "          rotate=(-45, 45),\n",
        "          shear=(-16, 16),\n",
        "          order=[0, 1],\n",
        "          cval=(0, 255),\n",
        "          mode=ia.ALL\n",
        "      )),\n",
        "      \n",
        "      #drop 2-5% percent of the original size, leading to large dropped\n",
        "      # rectangles.\n",
        "      sometimes(iaa.CoarseDropout(\n",
        "                        (0.03, 0.15), size_percent=(0.02, 0.05),\n",
        "                        per_channel=0.2\n",
        "                    )),\n",
        "                \n",
        "      # Make some images brighter and some darker.\n",
        "      # In 20% of all cases, we sample the multiplier once per channel,\n",
        "      # which can end up changing the color of the images.\n",
        "      sometimes(iaa.Multiply((0.8, 1.2), per_channel=0.2)),\n",
        "      \n",
        "      #Improve or worsen the contrast of images.\n",
        "      #Comment it out after third model run (extreme saturation)\n",
        "      sometimes(iaa.ContrastNormalization((0.75, 1.5), per_channel=0.5)), \n",
        "     ],\n",
        "     # do all of the above augmentations in random order\n",
        "     random_order = True) # apply augmenters in random order\n",
        "  \n",
        "  output_img = seq.augment_image(input_img)\n",
        "  return output_img\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255., preprocessing_function = CustomImageDataGen)\n",
        "valid_datagen = ImageDataGenerator(rescale=1/255.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsFcFmQckziT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory( r'./tiny-imagenet-200/train/', \n",
        "                                                    target_size=(img_width, img_height), \n",
        "                                                    batch_size=batch_size, \n",
        "                                                    class_mode='categorical', \n",
        "                                                    shuffle=True, seed=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO2bATZ3k1j5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_generator = valid_datagen.flow_from_dataframe(val_data, directory='./tiny-imagenet-200/val/images/', \n",
        "                                                         x_col='File', y_col='Class', \n",
        "                                                         target_size=(img_width, img_height),\n",
        "                                                         class_mode='categorical', \n",
        "                                                         batch_size=batch_size, \n",
        "                                                         shuffle=False, seed=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFffCOu6k20s",
        "colab_type": "text"
      },
      "source": [
        "# Model Compile and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSm5A3zpk5i_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the Model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer= keras.optimizers.Adam(lr= 0.0001, epsilon=1e-08),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqfn_nqlk6zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "# Callbacks\n",
        "clr = CyclicLR(base_lr=0.0001, max_lr=0.0006, step_size=4686., mode='triangular2') #Cyclic learning rate\n",
        "checkpointer = ModelCheckpoint(filepath=\"vgg16_tinyimagenet1.h5\", verbose=1, save_best_only=True, monitor=\"val_accuracy\")\n",
        "\n",
        "# Fit the Model\n",
        "model.fit_generator(train_generator,\n",
        "                    epochs=epochs,\n",
        "                    steps_per_epoch= num_train // batch_size,\n",
        "                    validation_steps= num_validation // batch_size,\n",
        "                    validation_data=validation_generator,\n",
        "                    verbose=1, callbacks=[clr, checkpointer]\n",
        "                   )\n",
        "model.save('vgg16_tinyimagenet1_final.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CKYC8Rok9Ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the model best model\n",
        "from keras.models import load_model\n",
        "new_model = load_model(\"vgg16_tinyimagenet1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzs4svpulUF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# callbacks\n",
        "clr = CyclicLR(base_lr=0.00001, max_lr=0.00006, step_size=4686., mode='triangular2') #Cyclic learning rate\n",
        "checkpointer_2 = ModelCheckpoint(filepath=\"vgg16_tinyimagenet2.h5\", verbose=1, save_best_only=True, monitor=\"val_accuracy\")\n",
        "# fit the model\n",
        "new_model.fit_generator(train_generator,\n",
        "                        epochs=12,\n",
        "                        steps_per_epoch= num_train // batch_size,\n",
        "                        validation_steps= num_validation // batch_size,\n",
        "                        validation_data=validation_generator,\n",
        "                        verbose=1, callbacks=[clr, checkpointer_2]\n",
        "                       )\n",
        "new_model.save('vgg16_tinyimagenet2_final.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrO759XvleuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the model best model\n",
        "from keras.models import load_model\n",
        "new_model = load_model(\"vgg16_tinyimagenet2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K64VmS4Nlf3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# callbacks\n",
        "clr = CyclicLR(base_lr=0.00001, max_lr=0.00006, step_size=4686., mode='triangular2') #Cyclic learning rate\n",
        "checkpointer_2 = ModelCheckpoint(filepath=\"vgg16_tinyimagenet3.h5\", verbose=1, save_best_only=True, monitor=\"val_accuracy\")\n",
        "# fit the model\n",
        "new_model.fit_generator(train_generator,\n",
        "                        epochs=12,\n",
        "                        steps_per_epoch= num_train // batch_size,\n",
        "                        validation_steps= num_validation // batch_size,\n",
        "                        validation_data=validation_generator,\n",
        "                        verbose=1, callbacks=[clr, checkpointer_2]\n",
        "                       )\n",
        "new_model.save('vgg16_tinyimagenet3_final.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cMOuOhwmqPQ",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrEpOeV2mrT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = extended_model.evaluate(validation_generator)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1]*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_X8bgLdnPG4",
        "colab_type": "text"
      },
      "source": [
        "# Testing on CCP Attack\n",
        "### Testing the CALTECH test dataset with s=1 and b=30\n",
        "#### (CCP_F and CCP_V)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BikXqsbhnTJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Model\n",
        "import keras\n",
        "model = keras.models.load_model('vgg16_tinyimagenet3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEVbsbRXpPbL",
        "colab_type": "text"
      },
      "source": [
        "##### Note: Generate the CCP attacked images on validation set and use them for evaluating the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQsqFRiMpP8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read Validation Dataset\n",
        "val_data_transformed = pd.read_csv('./tranformed-tiny-imagenet-200/val/val_annotations.txt', sep='\\t', header=None, \n",
        "                       names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "val_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True)\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "validation_generator_transformed = valid_datagen.flow_from_dataframe(val_data_transformed, directory='./tranformed-tiny-imagenet-200/val/images/', \n",
        "                                                         x_col='File', y_col='Class', \n",
        "                                                         target_size=(img_width, img_height),\n",
        "                                                         class_mode='categorical', \n",
        "                                                         batch_size=batch_size, \n",
        "                                                         shuffle=False, seed=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA39mxakpKwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = extended_model.evaluate(validation_generator_transformed)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1]*100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}